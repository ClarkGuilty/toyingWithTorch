{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 51125144.0\n",
      "1 59928544.0\n",
      "2 64748600.0\n",
      "3 49714032.0\n",
      "4 23611104.0\n",
      "5 8326498.0\n",
      "6 3517390.0\n",
      "7 2191487.25\n",
      "8 1668901.0\n",
      "9 1355988.125\n",
      "10 1126495.25\n",
      "11 946724.625\n",
      "12 801985.1875\n",
      "13 684283.0\n",
      "14 587596.875\n",
      "15 507428.34375\n",
      "16 440488.8125\n",
      "17 384161.4375\n",
      "18 336540.96875\n",
      "19 296016.09375\n",
      "20 261441.5\n",
      "21 231757.78125\n",
      "22 206102.5\n",
      "23 183825.09375\n",
      "24 164403.109375\n",
      "25 147413.375\n",
      "26 132489.484375\n",
      "27 119355.6171875\n",
      "28 107777.890625\n",
      "29 97529.875\n",
      "30 88424.421875\n",
      "31 80317.9609375\n",
      "32 73083.3515625\n",
      "33 66611.15625\n",
      "34 60809.3984375\n",
      "35 55599.46875\n",
      "36 50908.9140625\n",
      "37 46677.84375\n",
      "38 42855.94140625\n",
      "39 39395.9453125\n",
      "40 36256.49609375\n",
      "41 33404.9765625\n",
      "42 30810.265625\n",
      "43 28445.85546875\n",
      "44 26289.359375\n",
      "45 24318.90625\n",
      "46 22516.546875\n",
      "47 20866.27734375\n",
      "48 19355.6875\n",
      "49 17969.81640625\n",
      "50 16696.2421875\n",
      "51 15524.4462890625\n",
      "52 14445.6025390625\n",
      "53 13451.291015625\n",
      "54 12534.091796875\n",
      "55 11687.337890625\n",
      "56 10905.064453125\n",
      "57 10181.0966796875\n",
      "58 9511.060546875\n",
      "59 8890.4814453125\n",
      "60 8316.087890625\n",
      "61 7783.10400390625\n",
      "62 7288.365234375\n",
      "63 6828.6357421875\n",
      "64 6400.958984375\n",
      "65 6003.2275390625\n",
      "66 5632.96240234375\n",
      "67 5287.96875\n",
      "68 4966.3203125\n",
      "69 4666.72265625\n",
      "70 4387.07763671875\n",
      "71 4125.90283203125\n",
      "72 3881.888427734375\n",
      "73 3653.775390625\n",
      "74 3440.415283203125\n",
      "75 3240.80615234375\n",
      "76 3053.956298828125\n",
      "77 2878.918701171875\n",
      "78 2714.7685546875\n",
      "79 2560.85791015625\n",
      "80 2416.50390625\n",
      "81 2281.072021484375\n",
      "82 2153.924072265625\n",
      "83 2034.49609375\n",
      "84 1922.250244140625\n",
      "85 1816.77294921875\n",
      "86 1717.595703125\n",
      "87 1624.293212890625\n",
      "88 1536.4737548828125\n",
      "89 1453.853759765625\n",
      "90 1376.0360107421875\n",
      "91 1302.715576171875\n",
      "92 1233.6451416015625\n",
      "93 1168.5391845703125\n",
      "94 1107.15234375\n",
      "95 1049.2392578125\n",
      "96 994.6121215820312\n",
      "97 943.0695190429688\n",
      "98 894.4412231445312\n",
      "99 848.4743041992188\n",
      "100 805.0462036132812\n",
      "101 764.0196533203125\n",
      "102 725.2429809570312\n",
      "103 688.5972900390625\n",
      "104 653.9578857421875\n",
      "105 621.1785888671875\n",
      "106 590.1737670898438\n",
      "107 560.8417358398438\n",
      "108 533.06982421875\n",
      "109 506.7711486816406\n",
      "110 481.88238525390625\n",
      "111 458.3040466308594\n",
      "112 435.96630859375\n",
      "113 414.7843017578125\n",
      "114 394.71014404296875\n",
      "115 375.6754455566406\n",
      "116 357.6278381347656\n",
      "117 340.5168151855469\n",
      "118 324.2807312011719\n",
      "119 308.8673400878906\n",
      "120 294.2445068359375\n",
      "121 280.3599853515625\n",
      "122 267.17706298828125\n",
      "123 254.6546630859375\n",
      "124 242.7646484375\n",
      "125 231.4628448486328\n",
      "126 220.72373962402344\n",
      "127 210.5166473388672\n",
      "128 200.8137664794922\n",
      "129 191.5893096923828\n",
      "130 182.81881713867188\n",
      "131 174.47300720214844\n",
      "132 166.53912353515625\n",
      "133 158.98593139648438\n",
      "134 151.7955780029297\n",
      "135 144.95175170898438\n",
      "136 138.43760681152344\n",
      "137 132.2368927001953\n",
      "138 126.33088684082031\n",
      "139 120.70382690429688\n",
      "140 115.344482421875\n",
      "141 110.23699951171875\n",
      "142 105.36820983886719\n",
      "143 100.73126983642578\n",
      "144 96.30918884277344\n",
      "145 92.09304809570312\n",
      "146 88.0733871459961\n",
      "147 84.23854064941406\n",
      "148 80.5804672241211\n",
      "149 77.09408569335938\n",
      "150 73.765869140625\n",
      "151 70.58787536621094\n",
      "152 67.55646514892578\n",
      "153 64.66259002685547\n",
      "154 61.900272369384766\n",
      "155 59.26300811767578\n",
      "156 56.74460220336914\n",
      "157 54.33845520019531\n",
      "158 52.041534423828125\n",
      "159 49.845542907714844\n",
      "160 47.74853515625\n",
      "161 45.745174407958984\n",
      "162 43.82814407348633\n",
      "163 41.99729919433594\n",
      "164 40.246124267578125\n",
      "165 38.57259750366211\n",
      "166 36.9719352722168\n",
      "167 35.44258117675781\n",
      "168 33.979034423828125\n",
      "169 32.57972717285156\n",
      "170 31.239330291748047\n",
      "171 29.95862579345703\n",
      "172 28.73275375366211\n",
      "173 27.560176849365234\n",
      "174 26.437347412109375\n",
      "175 25.362226486206055\n",
      "176 24.333324432373047\n",
      "177 23.348369598388672\n",
      "178 22.40509605407715\n",
      "179 21.502750396728516\n",
      "180 20.638628005981445\n",
      "181 19.8101749420166\n",
      "182 19.016536712646484\n",
      "183 18.25672721862793\n",
      "184 17.52806854248047\n",
      "185 16.830875396728516\n",
      "186 16.16195297241211\n",
      "187 15.521015167236328\n",
      "188 14.906917572021484\n",
      "189 14.317769050598145\n",
      "190 13.753398895263672\n",
      "191 13.212442398071289\n",
      "192 12.693822860717773\n",
      "193 12.196200370788574\n",
      "194 11.718853950500488\n",
      "195 11.261382102966309\n",
      "196 10.822210311889648\n",
      "197 10.400938034057617\n",
      "198 9.9976167678833\n",
      "199 9.609636306762695\n",
      "200 9.237359046936035\n",
      "201 8.880705833435059\n",
      "202 8.538420677185059\n",
      "203 8.20927619934082\n",
      "204 7.893773078918457\n",
      "205 7.591142177581787\n",
      "206 7.3001275062561035\n",
      "207 7.02094841003418\n",
      "208 6.7524871826171875\n",
      "209 6.495028495788574\n",
      "210 6.247867584228516\n",
      "211 6.01019287109375\n",
      "212 5.782551288604736\n",
      "213 5.563244819641113\n",
      "214 5.352499485015869\n",
      "215 5.150632858276367\n",
      "216 4.956253528594971\n",
      "217 4.769451141357422\n",
      "218 4.5901899337768555\n",
      "219 4.417750358581543\n",
      "220 4.252083778381348\n",
      "221 4.0929975509643555\n",
      "222 3.939883232116699\n",
      "223 3.7925760746002197\n",
      "224 3.651283025741577\n",
      "225 3.515162467956543\n",
      "226 3.384620189666748\n",
      "227 3.258908748626709\n",
      "228 3.1381993293762207\n",
      "229 3.021853446960449\n",
      "230 2.9100379943847656\n",
      "231 2.8024353981018066\n",
      "232 2.6990928649902344\n",
      "233 2.5995607376098633\n",
      "234 2.5039479732513428\n",
      "235 2.411820411682129\n",
      "236 2.3233580589294434\n",
      "237 2.2380878925323486\n",
      "238 2.1563048362731934\n",
      "239 2.0773959159851074\n",
      "240 2.0014586448669434\n",
      "241 1.9283533096313477\n",
      "242 1.8580366373062134\n",
      "243 1.7903560400009155\n",
      "244 1.7252534627914429\n",
      "245 1.6625123023986816\n",
      "246 1.6021568775177002\n",
      "247 1.5441969633102417\n",
      "248 1.4883065223693848\n",
      "249 1.434354305267334\n",
      "250 1.3825763463974\n",
      "251 1.332645297050476\n",
      "252 1.2847356796264648\n",
      "253 1.238445520401001\n",
      "254 1.1938093900680542\n",
      "255 1.1508357524871826\n",
      "256 1.1095397472381592\n",
      "257 1.0696722269058228\n",
      "258 1.0314016342163086\n",
      "259 0.9943789839744568\n",
      "260 0.9588900208473206\n",
      "261 0.9245889186859131\n",
      "262 0.8915917873382568\n",
      "263 0.8597009181976318\n",
      "264 0.8290717601776123\n",
      "265 0.7996311187744141\n",
      "266 0.7710494995117188\n",
      "267 0.743751585483551\n",
      "268 0.7172807455062866\n",
      "269 0.6918123960494995\n",
      "270 0.6673349142074585\n",
      "271 0.6436526775360107\n",
      "272 0.620864987373352\n",
      "273 0.5989878177642822\n",
      "274 0.5778108835220337\n",
      "275 0.5574105978012085\n",
      "276 0.5376959443092346\n",
      "277 0.518749475479126\n",
      "278 0.5004914999008179\n",
      "279 0.48287689685821533\n",
      "280 0.465889573097229\n",
      "281 0.4495021402835846\n",
      "282 0.4337467551231384\n",
      "283 0.4185475707054138\n",
      "284 0.4039273262023926\n",
      "285 0.3897371292114258\n",
      "286 0.376065731048584\n",
      "287 0.3629123270511627\n",
      "288 0.3502327799797058\n",
      "289 0.3380097448825836\n",
      "290 0.32618987560272217\n",
      "291 0.3147643804550171\n",
      "292 0.30384644865989685\n",
      "293 0.2932665944099426\n",
      "294 0.2830491364002228\n",
      "295 0.27318480610847473\n",
      "296 0.26374351978302\n",
      "297 0.25456559658050537\n",
      "298 0.24573078751564026\n",
      "299 0.23719877004623413\n",
      "300 0.22899512946605682\n",
      "301 0.22106921672821045\n",
      "302 0.21339207887649536\n",
      "303 0.20601026713848114\n",
      "304 0.19889041781425476\n",
      "305 0.19202153384685516\n",
      "306 0.18542785942554474\n",
      "307 0.17903034389019012\n",
      "308 0.17285187542438507\n",
      "309 0.16687265038490295\n",
      "310 0.16108438372612\n",
      "311 0.15554094314575195\n",
      "312 0.15019963681697845\n",
      "313 0.14502304792404175\n",
      "314 0.14007967710494995\n",
      "315 0.1352616399526596\n",
      "316 0.13059911131858826\n",
      "317 0.12612539529800415\n",
      "318 0.12180276215076447\n",
      "319 0.11762861162424088\n",
      "320 0.11356047540903091\n",
      "321 0.10968618094921112\n",
      "322 0.10592401027679443\n",
      "323 0.10233038663864136\n",
      "324 0.09882329404354095\n",
      "325 0.0954364538192749\n",
      "326 0.09219164401292801\n",
      "327 0.08904869109392166\n",
      "328 0.08601713180541992\n",
      "329 0.08309611678123474\n",
      "330 0.08025143295526505\n",
      "331 0.0775168240070343\n",
      "332 0.07485659420490265\n",
      "333 0.07233260571956635\n",
      "334 0.0698835551738739\n",
      "335 0.06750234216451645\n",
      "336 0.06520050764083862\n",
      "337 0.06299424916505814\n",
      "338 0.06084659695625305\n",
      "339 0.058801569044589996\n",
      "340 0.05681605264544487\n",
      "341 0.05489598959684372\n",
      "342 0.0530267208814621\n",
      "343 0.05123183876276016\n",
      "344 0.04949379712343216\n",
      "345 0.047822512686252594\n",
      "346 0.046208456158638\n",
      "347 0.04464961588382721\n",
      "348 0.04314876347780228\n",
      "349 0.04169383645057678\n",
      "350 0.0402887798845768\n",
      "351 0.03893500939011574\n",
      "352 0.03763377666473389\n",
      "353 0.036360401660203934\n",
      "354 0.0351446308195591\n",
      "355 0.03395257145166397\n",
      "356 0.03280732035636902\n",
      "357 0.03173404186964035\n",
      "358 0.030654827132821083\n",
      "359 0.029630417004227638\n",
      "360 0.028652310371398926\n",
      "361 0.027692612260580063\n",
      "362 0.026777055114507675\n",
      "363 0.025882570073008537\n",
      "364 0.025024764239788055\n",
      "365 0.02419394440948963\n",
      "366 0.02338523045182228\n",
      "367 0.022604070603847504\n",
      "368 0.021855289116501808\n",
      "369 0.021130554378032684\n",
      "370 0.020431717857718468\n",
      "371 0.01974450796842575\n",
      "372 0.01909538358449936\n",
      "373 0.018469367176294327\n",
      "374 0.017865054309368134\n",
      "375 0.017273586243391037\n",
      "376 0.016701241955161095\n",
      "377 0.016153555363416672\n",
      "378 0.0156282689422369\n",
      "379 0.01511481311172247\n",
      "380 0.014624682255089283\n",
      "381 0.014142248779535294\n",
      "382 0.013678159564733505\n",
      "383 0.013227835297584534\n",
      "384 0.01280345767736435\n",
      "385 0.012383222579956055\n",
      "386 0.011983082629740238\n",
      "387 0.011590678244829178\n",
      "388 0.011220531538128853\n",
      "389 0.010854814201593399\n",
      "390 0.01050511747598648\n",
      "391 0.010159141384065151\n",
      "392 0.009840764105319977\n",
      "393 0.009527270682156086\n",
      "394 0.009223619475960732\n",
      "395 0.008926521986722946\n",
      "396 0.008641388267278671\n",
      "397 0.00836469791829586\n",
      "398 0.008098759688436985\n",
      "399 0.007838582620024681\n",
      "400 0.007588018663227558\n",
      "401 0.007346644997596741\n",
      "402 0.007117732893675566\n",
      "403 0.006890522316098213\n",
      "404 0.0066732559353113174\n",
      "405 0.0064638168551027775\n",
      "406 0.006260549649596214\n",
      "407 0.00606963038444519\n",
      "408 0.005881876684725285\n",
      "409 0.005697897169739008\n",
      "410 0.0055191731080412865\n",
      "411 0.005348344333469868\n",
      "412 0.00518307788297534\n",
      "413 0.0050248983316123486\n",
      "414 0.0048731667920947075\n",
      "415 0.004723978228867054\n",
      "416 0.004576569888740778\n",
      "417 0.004436484072357416\n",
      "418 0.0043024420738220215\n",
      "419 0.0041769579984247684\n",
      "420 0.0040505737997591496\n",
      "421 0.003929791506379843\n",
      "422 0.003813458839431405\n",
      "423 0.0036972397938370705\n",
      "424 0.003588041989132762\n",
      "425 0.0034814970567822456\n",
      "426 0.003377179615199566\n",
      "427 0.0032759709283709526\n",
      "428 0.0031818614806979895\n",
      "429 0.0030894074589014053\n",
      "430 0.0029990929178893566\n",
      "431 0.0029107097070664167\n",
      "432 0.0028312988579273224\n",
      "433 0.0027486521285027266\n",
      "434 0.002667372114956379\n",
      "435 0.0025933461729437113\n",
      "436 0.002521281596273184\n",
      "437 0.002448159968480468\n",
      "438 0.0023791908752173185\n",
      "439 0.0023141445126384497\n",
      "440 0.002246942836791277\n",
      "441 0.002184283919632435\n",
      "442 0.0021238536573946476\n",
      "443 0.0020680830348283052\n",
      "444 0.0020104050636291504\n",
      "445 0.0019558726344257593\n",
      "446 0.001905941404402256\n",
      "447 0.0018515917472541332\n",
      "448 0.0018015594687312841\n",
      "449 0.0017529877368360758\n",
      "450 0.0017061308026313782\n",
      "451 0.0016617713263258338\n",
      "452 0.0016147340647876263\n",
      "453 0.0015727465506643057\n",
      "454 0.0015335038769990206\n",
      "455 0.0014925419818609953\n",
      "456 0.0014541468117386103\n",
      "457 0.0014148681657388806\n",
      "458 0.0013803720939904451\n",
      "459 0.0013454743893817067\n",
      "460 0.0013101401273161173\n",
      "461 0.0012774105416610837\n",
      "462 0.0012465177569538355\n",
      "463 0.001213278155773878\n",
      "464 0.0011841332307085395\n",
      "465 0.0011556355748325586\n",
      "466 0.0011268469970673323\n",
      "467 0.0010987423593178391\n",
      "468 0.0010732486844062805\n",
      "469 0.0010465604718774557\n",
      "470 0.0010212813504040241\n",
      "471 0.0009977980516850948\n",
      "472 0.0009736923384480178\n",
      "473 0.0009502910543233156\n",
      "474 0.0009277991484850645\n",
      "475 0.0009065362391993403\n",
      "476 0.0008861042442731559\n",
      "477 0.0008660434978082776\n",
      "478 0.0008454907219856977\n",
      "479 0.0008261637995019555\n",
      "480 0.0008097001118585467\n",
      "481 0.0007906848913989961\n",
      "482 0.0007739132270216942\n",
      "483 0.0007554274052381516\n",
      "484 0.0007384346681647003\n",
      "485 0.000723770703189075\n",
      "486 0.0007076173787936568\n",
      "487 0.0006935451528988779\n",
      "488 0.0006759750540368259\n",
      "489 0.0006631483556702733\n",
      "490 0.0006492309621535242\n",
      "491 0.0006359390681609511\n",
      "492 0.0006217834306880832\n",
      "493 0.0006086258217692375\n",
      "494 0.0005962826544418931\n",
      "495 0.0005840421654284\n",
      "496 0.0005714591243304312\n",
      "497 0.000559768290258944\n",
      "498 0.0005484531284309924\n",
      "499 0.0005370291182771325\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 648.375732421875\n",
      "1 593.3486938476562\n",
      "2 546.3768310546875\n",
      "3 505.7945251464844\n",
      "4 470.1651916503906\n",
      "5 438.6766052246094\n",
      "6 410.1092834472656\n",
      "7 384.1275939941406\n",
      "8 360.4639587402344\n",
      "9 338.6230773925781\n",
      "10 318.34027099609375\n",
      "11 299.44189453125\n",
      "12 281.78741455078125\n",
      "13 265.1520080566406\n",
      "14 249.52139282226562\n",
      "15 234.8289337158203\n",
      "16 221.0250244140625\n",
      "17 207.96923828125\n",
      "18 195.61349487304688\n",
      "19 183.91159057617188\n",
      "20 172.83074951171875\n",
      "21 162.37367248535156\n",
      "22 152.5302276611328\n",
      "23 143.24705505371094\n",
      "24 134.44195556640625\n",
      "25 126.1460952758789\n",
      "26 118.33324432373047\n",
      "27 110.98616790771484\n",
      "28 104.07931518554688\n",
      "29 97.60888671875\n",
      "30 91.52758026123047\n",
      "31 85.81489562988281\n",
      "32 80.47791290283203\n",
      "33 75.4676284790039\n",
      "34 70.77373504638672\n",
      "35 66.35245513916016\n",
      "36 62.208736419677734\n",
      "37 58.318538665771484\n",
      "38 54.66971206665039\n",
      "39 51.23996353149414\n",
      "40 48.029720306396484\n",
      "41 45.017818450927734\n",
      "42 42.19574737548828\n",
      "43 39.552398681640625\n",
      "44 37.07732391357422\n",
      "45 34.75766372680664\n",
      "46 32.58528137207031\n",
      "47 30.551700592041016\n",
      "48 28.646692276000977\n",
      "49 26.86489486694336\n",
      "50 25.197101593017578\n",
      "51 23.638460159301758\n",
      "52 22.178333282470703\n",
      "53 20.813865661621094\n",
      "54 19.539501190185547\n",
      "55 18.350400924682617\n",
      "56 17.237197875976562\n",
      "57 16.195398330688477\n",
      "58 15.221446990966797\n",
      "59 14.30999755859375\n",
      "60 13.456160545349121\n",
      "61 12.656234741210938\n",
      "62 11.907736778259277\n",
      "63 11.206066131591797\n",
      "64 10.548527717590332\n",
      "65 9.932565689086914\n",
      "66 9.355324745178223\n",
      "67 8.814250946044922\n",
      "68 8.30652141571045\n",
      "69 7.830704212188721\n",
      "70 7.383753776550293\n",
      "71 6.964856147766113\n",
      "72 6.571933269500732\n",
      "73 6.203651428222656\n",
      "74 5.85788106918335\n",
      "75 5.5334553718566895\n",
      "76 5.228584289550781\n",
      "77 4.941660404205322\n",
      "78 4.671948432922363\n",
      "79 4.418260097503662\n",
      "80 4.179776668548584\n",
      "81 3.9550018310546875\n",
      "82 3.7436776161193848\n",
      "83 3.5447051525115967\n",
      "84 3.356682538986206\n",
      "85 3.1793904304504395\n",
      "86 3.012378215789795\n",
      "87 2.8550100326538086\n",
      "88 2.706937074661255\n",
      "89 2.5673179626464844\n",
      "90 2.435452938079834\n",
      "91 2.311100959777832\n",
      "92 2.1937737464904785\n",
      "93 2.0828664302825928\n",
      "94 1.978352665901184\n",
      "95 1.879953384399414\n",
      "96 1.7867661714553833\n",
      "97 1.698589563369751\n",
      "98 1.6151739358901978\n",
      "99 1.5362660884857178\n",
      "100 1.4616203308105469\n",
      "101 1.3908177614212036\n",
      "102 1.3236197233200073\n",
      "103 1.2599143981933594\n",
      "104 1.1995365619659424\n",
      "105 1.1423383951187134\n",
      "106 1.0881015062332153\n",
      "107 1.0366168022155762\n",
      "108 0.9877653121948242\n",
      "109 0.9414311051368713\n",
      "110 0.897456705570221\n",
      "111 0.8557187914848328\n",
      "112 0.816095769405365\n",
      "113 0.7784258127212524\n",
      "114 0.7426527142524719\n",
      "115 0.7086857557296753\n",
      "116 0.6763816475868225\n",
      "117 0.6456528902053833\n",
      "118 0.6164147853851318\n",
      "119 0.5886467695236206\n",
      "120 0.5621973872184753\n",
      "121 0.5370351076126099\n",
      "122 0.5130845308303833\n",
      "123 0.4902876913547516\n",
      "124 0.4685908555984497\n",
      "125 0.44792407751083374\n",
      "126 0.4282365143299103\n",
      "127 0.40947386622428894\n",
      "128 0.3915975093841553\n",
      "129 0.37456920742988586\n",
      "130 0.3583616018295288\n",
      "131 0.3428952693939209\n",
      "132 0.3281548321247101\n",
      "133 0.31407877802848816\n",
      "134 0.3006727993488312\n",
      "135 0.2878783643245697\n",
      "136 0.2756681442260742\n",
      "137 0.26401710510253906\n",
      "138 0.25287356972694397\n",
      "139 0.24223889410495758\n",
      "140 0.23207823932170868\n",
      "141 0.22234326601028442\n",
      "142 0.21305319666862488\n",
      "143 0.20416787266731262\n",
      "144 0.1956847757101059\n",
      "145 0.18757396936416626\n",
      "146 0.17981553077697754\n",
      "147 0.17240187525749207\n",
      "148 0.16531558334827423\n",
      "149 0.158534437417984\n",
      "150 0.1520390659570694\n",
      "151 0.1458292007446289\n",
      "152 0.13989442586898804\n",
      "153 0.1342206597328186\n",
      "154 0.12879173457622528\n",
      "155 0.12359452992677689\n",
      "156 0.1186361014842987\n",
      "157 0.1138966754078865\n",
      "158 0.10935350507497787\n",
      "159 0.10500600934028625\n",
      "160 0.10084137320518494\n",
      "161 0.09685716778039932\n",
      "162 0.09303673356771469\n",
      "163 0.08936953544616699\n",
      "164 0.08585541695356369\n",
      "165 0.08248981088399887\n",
      "166 0.07926586270332336\n",
      "167 0.07617224752902985\n",
      "168 0.07320559024810791\n",
      "169 0.07036145776510239\n",
      "170 0.06763488799333572\n",
      "171 0.06501785665750504\n",
      "172 0.06250932812690735\n",
      "173 0.06010304391384125\n",
      "174 0.05779385566711426\n",
      "175 0.05557834357023239\n",
      "176 0.05345208942890167\n",
      "177 0.051413606852293015\n",
      "178 0.04945427551865578\n",
      "179 0.04757321625947952\n",
      "180 0.045767415314912796\n",
      "181 0.044034283608198166\n",
      "182 0.04236993193626404\n",
      "183 0.040770273655653\n",
      "184 0.0392349511384964\n",
      "185 0.037759602069854736\n",
      "186 0.03634300455451012\n",
      "187 0.03498280793428421\n",
      "188 0.033675417304039\n",
      "189 0.032418228685855865\n",
      "190 0.03121001273393631\n",
      "191 0.030050037428736687\n",
      "192 0.02893439307808876\n",
      "193 0.027862003073096275\n",
      "194 0.02683204412460327\n",
      "195 0.025840317830443382\n",
      "196 0.02488723210990429\n",
      "197 0.02397114783525467\n",
      "198 0.023090146481990814\n",
      "199 0.022242562845349312\n",
      "200 0.021427586674690247\n",
      "201 0.020643489435315132\n",
      "202 0.019889596849679947\n",
      "203 0.019164232537150383\n",
      "204 0.018466206267476082\n",
      "205 0.017794327810406685\n",
      "206 0.0171490591019392\n",
      "207 0.01652766764163971\n",
      "208 0.015929779037833214\n",
      "209 0.015353905968368053\n",
      "210 0.01479987520724535\n",
      "211 0.014266978949308395\n",
      "212 0.013753682374954224\n",
      "213 0.013259650208055973\n",
      "214 0.012784115970134735\n",
      "215 0.012326185591518879\n",
      "216 0.011884994804859161\n",
      "217 0.01146023254841566\n",
      "218 0.01105132419615984\n",
      "219 0.010657737031579018\n",
      "220 0.010278621688485146\n",
      "221 0.009913181886076927\n",
      "222 0.009561030194163322\n",
      "223 0.009222078137099743\n",
      "224 0.008895437233150005\n",
      "225 0.008580847643315792\n",
      "226 0.008277914486825466\n",
      "227 0.007985968142747879\n",
      "228 0.007704510353505611\n",
      "229 0.007433184888213873\n",
      "230 0.007171653211116791\n",
      "231 0.006919898092746735\n",
      "232 0.006677157711237669\n",
      "233 0.006443350110203028\n",
      "234 0.006217892747372389\n",
      "235 0.0060005211271345615\n",
      "236 0.005790949333459139\n",
      "237 0.005589031148701906\n",
      "238 0.005394262727349997\n",
      "239 0.005206525791436434\n",
      "240 0.005025460384786129\n",
      "241 0.00485101668164134\n",
      "242 0.004682670813053846\n",
      "243 0.004520670976489782\n",
      "244 0.004364142660051584\n",
      "245 0.004213146865367889\n",
      "246 0.00406764866784215\n",
      "247 0.003927510231733322\n",
      "248 0.0037922407500445843\n",
      "249 0.003661645110696554\n",
      "250 0.0035356481093913317\n",
      "251 0.003414166159927845\n",
      "252 0.0032970882020890713\n",
      "253 0.0031840866431593895\n",
      "254 0.0030750075820833445\n",
      "255 0.0029697364661842585\n",
      "256 0.002868225099518895\n",
      "257 0.0027702520601451397\n",
      "258 0.0026756918523460627\n",
      "259 0.002584426198154688\n",
      "260 0.0024963803589344025\n",
      "261 0.0024114111438393593\n",
      "262 0.0023294806014746428\n",
      "263 0.002250307472422719\n",
      "264 0.002173999324440956\n",
      "265 0.00210033287294209\n",
      "266 0.0020291805267333984\n",
      "267 0.0019604340195655823\n",
      "268 0.0018940813606604934\n",
      "269 0.0018301015952602029\n",
      "270 0.0017683309270069003\n",
      "271 0.0017086919397115707\n",
      "272 0.001651083119213581\n",
      "273 0.001595485839061439\n",
      "274 0.0015417917165905237\n",
      "275 0.001489925547502935\n",
      "276 0.0014398640487343073\n",
      "277 0.0013915166491642594\n",
      "278 0.0013448125682771206\n",
      "279 0.0012997019803151488\n",
      "280 0.0012561725452542305\n",
      "281 0.0012141084298491478\n",
      "282 0.0011734795989468694\n",
      "283 0.0011342631187289953\n",
      "284 0.0010964436223730445\n",
      "285 0.0010598561493679881\n",
      "286 0.001024508150294423\n",
      "287 0.0009903600439429283\n",
      "288 0.0009574084542691708\n",
      "289 0.0009255534969270229\n",
      "290 0.0008947731694206595\n",
      "291 0.0008650533854961395\n",
      "292 0.0008363134111277759\n",
      "293 0.000808541604783386\n",
      "294 0.0007817497826181352\n",
      "295 0.0007558557554148138\n",
      "296 0.0007308381027542055\n",
      "297 0.0007066582329571247\n",
      "298 0.0006832916405983269\n",
      "299 0.000660739722661674\n",
      "300 0.000638929835986346\n",
      "301 0.0006178430048748851\n",
      "302 0.0005974728264845908\n",
      "303 0.0005777784972451627\n",
      "304 0.0005587972700595856\n",
      "305 0.0005404192488640547\n",
      "306 0.000522638380061835\n",
      "307 0.0005054585635662079\n",
      "308 0.0004888527328148484\n",
      "309 0.00047280866419896483\n",
      "310 0.0004572859616018832\n",
      "311 0.0004422966740094125\n",
      "312 0.00042780840885825455\n",
      "313 0.00041381028131581843\n",
      "314 0.0004002736241091043\n",
      "315 0.0003871762310154736\n",
      "316 0.00037452115793712437\n",
      "317 0.0003622910298872739\n",
      "318 0.00035046483390033245\n",
      "319 0.00033903983421623707\n",
      "320 0.00032797938911244273\n",
      "321 0.00031729196780361235\n",
      "322 0.0003069599042646587\n",
      "323 0.0002969666966237128\n",
      "324 0.0002873111516237259\n",
      "325 0.00027798383962363005\n",
      "326 0.00026895033079199493\n",
      "327 0.00026023248210549355\n",
      "328 0.0002517896646168083\n",
      "329 0.00024362355179619044\n",
      "330 0.0002357229241169989\n",
      "331 0.0002280799235450104\n",
      "332 0.0002207022625952959\n",
      "333 0.00021356520301196724\n",
      "334 0.0002066615707008168\n",
      "335 0.000199977817828767\n",
      "336 0.00019351507944520563\n",
      "337 0.000187266748980619\n",
      "338 0.00018122010806109756\n",
      "339 0.00017537898384034634\n",
      "340 0.0001697241677902639\n",
      "341 0.0001642581628402695\n",
      "342 0.00015896977856755257\n",
      "343 0.00015385150618385524\n",
      "344 0.00014891014143358916\n",
      "345 0.00014411989832296968\n",
      "346 0.0001394954015268013\n",
      "347 0.00013501578359864652\n",
      "348 0.00013068107364233583\n",
      "349 0.00012648657138925046\n",
      "350 0.00012243047240190208\n",
      "351 0.000118510055472143\n",
      "352 0.00011471235484350473\n",
      "353 0.0001110408702515997\n",
      "354 0.00010749146895250306\n",
      "355 0.00010405575449112803\n",
      "356 0.00010073266457766294\n",
      "357 9.751220932230353e-05\n",
      "358 9.439801215194166e-05\n",
      "359 9.138325549429283e-05\n",
      "360 8.846570563036948e-05\n",
      "361 8.564489689888433e-05\n",
      "362 8.291460835607722e-05\n",
      "363 8.027200237847865e-05\n",
      "364 7.771883974783123e-05\n",
      "365 7.524163083871827e-05\n",
      "366 7.285297760972753e-05\n",
      "367 7.053359149722382e-05\n",
      "368 6.829207268310711e-05\n",
      "369 6.61247395328246e-05\n",
      "370 6.402584403986111e-05\n",
      "371 6.199486233526841e-05\n",
      "372 6.002808004268445e-05\n",
      "373 5.8123714552493766e-05\n",
      "374 5.628287181025371e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 5.449669333756901e-05\n",
      "376 5.2770774345844984e-05\n",
      "377 5.110029815114103e-05\n",
      "378 4.948389687342569e-05\n",
      "379 4.791935862158425e-05\n",
      "380 4.640449697035365e-05\n",
      "381 4.493688902584836e-05\n",
      "382 4.351804091129452e-05\n",
      "383 4.214571163174696e-05\n",
      "384 4.0817940316628665e-05\n",
      "385 3.9531565562356263e-05\n",
      "386 3.82869147870224e-05\n",
      "387 3.707958967424929e-05\n",
      "388 3.591357381083071e-05\n",
      "389 3.4780630812747404e-05\n",
      "390 3.3685792004689574e-05\n",
      "391 3.262706013629213e-05\n",
      "392 3.160020787618123e-05\n",
      "393 3.060900780837983e-05\n",
      "394 2.9647912015207112e-05\n",
      "395 2.871428478101734e-05\n",
      "396 2.7813413908006623e-05\n",
      "397 2.6942871045321226e-05\n",
      "398 2.6094745408045128e-05\n",
      "399 2.5277795430156402e-05\n",
      "400 2.448589657433331e-05\n",
      "401 2.371748814766761e-05\n",
      "402 2.2975409592618234e-05\n",
      "403 2.2256936063058674e-05\n",
      "404 2.1560124878305942e-05\n",
      "405 2.088635301333852e-05\n",
      "406 2.0232224414939992e-05\n",
      "407 1.959916335181333e-05\n",
      "408 1.8989168893313035e-05\n",
      "409 1.8397167877992615e-05\n",
      "410 1.7823906091507524e-05\n",
      "411 1.726588379824534e-05\n",
      "412 1.672713551670313e-05\n",
      "413 1.62073592946399e-05\n",
      "414 1.5701547454227693e-05\n",
      "415 1.5212756807159167e-05\n",
      "416 1.4738754543941468e-05\n",
      "417 1.4279442439146806e-05\n",
      "418 1.3835811842000112e-05\n",
      "419 1.3405217941908631e-05\n",
      "420 1.2987849913770333e-05\n",
      "421 1.2584851901920047e-05\n",
      "422 1.2194785085739568e-05\n",
      "423 1.1816247933893465e-05\n",
      "424 1.1449481462477706e-05\n",
      "425 1.1093097782577388e-05\n",
      "426 1.0750099136203062e-05\n",
      "427 1.0415390534035396e-05\n",
      "428 1.0093355740536936e-05\n",
      "429 9.780645996215753e-06\n",
      "430 9.476873856328893e-06\n",
      "431 9.184791451843921e-06\n",
      "432 8.900217835616786e-06\n",
      "433 8.625273039797321e-06\n",
      "434 8.357236765732523e-06\n",
      "435 8.098848411464132e-06\n",
      "436 7.849776011426002e-06\n",
      "437 7.607076895510545e-06\n",
      "438 7.371487299678847e-06\n",
      "439 7.144205483200494e-06\n",
      "440 6.92342246111366e-06\n",
      "441 6.710073193971766e-06\n",
      "442 6.503120403067442e-06\n",
      "443 6.302677775238408e-06\n",
      "444 6.108205070631811e-06\n",
      "445 5.920726380281849e-06\n",
      "446 5.738198979088338e-06\n",
      "447 5.562008027482079e-06\n",
      "448 5.3903272601019125e-06\n",
      "449 5.224301276030019e-06\n",
      "450 5.063451681053266e-06\n",
      "451 4.908571099804249e-06\n",
      "452 4.757373972097412e-06\n",
      "453 4.6112131713016424e-06\n",
      "454 4.469823124964023e-06\n",
      "455 4.332881417212775e-06\n",
      "456 4.199504019197775e-06\n",
      "457 4.071009243489243e-06\n",
      "458 3.945714524888899e-06\n",
      "459 3.824940449703718e-06\n",
      "460 3.7073275507282233e-06\n",
      "461 3.594038162191282e-06\n",
      "462 3.4839611089410027e-06\n",
      "463 3.3774749681469984e-06\n",
      "464 3.274674099884578e-06\n",
      "465 3.1738677535031457e-06\n",
      "466 3.0772455374972196e-06\n",
      "467 2.9829866434738506e-06\n",
      "468 2.892274324040045e-06\n",
      "469 2.803158622555202e-06\n",
      "470 2.7175547074875794e-06\n",
      "471 2.6347474886279088e-06\n",
      "472 2.554247203079285e-06\n",
      "473 2.4765190573816653e-06\n",
      "474 2.4009618755371775e-06\n",
      "475 2.328012897123699e-06\n",
      "476 2.256701691294438e-06\n",
      "477 2.188049165852135e-06\n",
      "478 2.1214264052105136e-06\n",
      "479 2.056545554296463e-06\n",
      "480 1.993837258851272e-06\n",
      "481 1.9336187051521847e-06\n",
      "482 1.8744478893495398e-06\n",
      "483 1.8174029037254513e-06\n",
      "484 1.762288320605876e-06\n",
      "485 1.7088157164835138e-06\n",
      "486 1.6563461713303695e-06\n",
      "487 1.6063258954090998e-06\n",
      "488 1.5576109717585496e-06\n",
      "489 1.5103930763871176e-06\n",
      "490 1.464490878788638e-06\n",
      "491 1.4200154510035645e-06\n",
      "492 1.3770289797321311e-06\n",
      "493 1.3350959306990262e-06\n",
      "494 1.2948323728778632e-06\n",
      "495 1.2551289501061547e-06\n",
      "496 1.217290218846756e-06\n",
      "497 1.180410549750377e-06\n",
      "498 1.144634666161437e-06\n",
      "499 1.1102162034148932e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    #print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011276787000750836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1b092df978>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "data = np.loadtxt('fitting.txt')\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "x1 , y1 = data[:,0],data[:,1]\n",
    "x,y = torch.from_numpy(np.asmatrix(x1).T),torch.from_numpy(np.asmatrix(y1).T)\n",
    "plt.scatter(x1,y1, label = 'data')\n",
    "#x, y = x1.double(), y1.double()\n",
    "\n",
    "\n",
    "N, D_in, H, D_out = len(x), 1, 350, 1\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(5000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    #print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "plt.scatter(x,y_pred.data, label = 'pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Identification MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "(28000, 784)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline  \n",
    "\n",
    "#Train data.\n",
    "train = pd.read_csv(\"input/train.csv\")\n",
    "print(train.shape)\n",
    "train.head()\n",
    "\n",
    "#Test data.\n",
    "test= pd.read_csv(\"input/test.csv\")\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28000, 28, 28, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = (train.iloc[:,1:].values).astype('float32') # all pixel values\n",
    "y_train = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\n",
    "X_test = test.values.astype('float32')\n",
    "print(len(X_train))\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28) #Un arreglo de 42000 imagenes de 28 x 28\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = 6\n",
    "for i in range(j, j+3):\n",
    "    plt.subplot(330 + (i+1))\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.title(y_train[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Media 0 y sigma 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32)\n",
    "\n",
    "def standardize(x): \n",
    "    return (x-mean_px)/std_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42000, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = train.iloc[:,0].values.astype('int32') #En caso de que por el orden de ejecucin del notebook haya cambiado.\n",
    "y_train= to_categorical(y_train)\n",
    "num_classes = y_train.shape[1]\n",
    "y_train = torch.from_numpy(y_train)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 43\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = (train.iloc[:,1:].values).astype('float32') # all pixel values\n",
    "y_train = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\n",
    "X_test = test.values.astype('float32')\n",
    "print(len(X_train))\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0], 28, 28) #Un arreglo de 42000 imagenes de 28 x 28\n",
    "#X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
    "X_test.shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Cordero(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cordero, self).__init__()\n",
    "        self.standard = standardize\n",
    "\n",
    "    def forward(self, x):\n",
    "       x = F.relu(self.standard(x))\n",
    "       return x\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 51519GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1544199946412/work/aten/src/TH/THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-cf35c028876b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mCordero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#inputs = X_train.view(42000,1,28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 51519GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1544199946412/work/aten/src/TH/THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "#Debo volver aqu cuando entienda ms.\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    Cordero(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(42000*28*28,42000*10),\n",
    ")\n",
    "#inputs = X_train.view(42000,1,28, 28)\n",
    "#print(inputs[0][0])\n",
    "model.to(device)\n",
    "inputs = X_train.contiguous().view(-1).to(device)\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(inputs)\n",
    "    out = y_pred.view(42000,10)\n",
    "    print(y_pred.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32928000/28/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
