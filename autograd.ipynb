{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27837480.0\n",
      "1 25254832.0\n",
      "2 26314180.0\n",
      "3 27313620.0\n",
      "4 25381572.0\n",
      "5 19952948.0\n",
      "6 13162245.0\n",
      "7 7633493.0\n",
      "8 4212695.0\n",
      "9 2400927.0\n",
      "10 1494110.125\n",
      "11 1031749.4375\n",
      "12 778080.75\n",
      "13 623508.75\n",
      "14 518768.53125\n",
      "15 441439.8125\n",
      "16 380815.78125\n",
      "17 331489.90625\n",
      "18 290372.09375\n",
      "19 255638.890625\n",
      "20 226114.125\n",
      "21 200759.46875\n",
      "22 178818.78125\n",
      "23 159800.46875\n",
      "24 143212.3125\n",
      "25 128686.140625\n",
      "26 115918.109375\n",
      "27 104659.140625\n",
      "28 94708.5078125\n",
      "29 85898.0546875\n",
      "30 78061.09375\n",
      "31 71079.1484375\n",
      "32 64838.859375\n",
      "33 59246.08203125\n",
      "34 54222.5390625\n",
      "35 49700.73828125\n",
      "36 45621.3828125\n",
      "37 41935.10546875\n",
      "38 38597.46875\n",
      "39 35571.47265625\n",
      "40 32821.65625\n",
      "41 30319.609375\n",
      "42 28039.46484375\n",
      "43 25958.76171875\n",
      "44 24057.31640625\n",
      "45 22316.9140625\n",
      "46 20722.271484375\n",
      "47 19258.681640625\n",
      "48 17914.53125\n",
      "49 16677.546875\n",
      "50 15538.626953125\n",
      "51 14488.66796875\n",
      "52 13520.130859375\n",
      "53 12624.90234375\n",
      "54 11797.259765625\n",
      "55 11030.875\n",
      "56 10320.896484375\n",
      "57 9662.544921875\n",
      "58 9051.9716796875\n",
      "59 8484.9970703125\n",
      "60 7958.0263671875\n",
      "61 7467.51904296875\n",
      "62 7010.978515625\n",
      "63 6585.7265625\n",
      "64 6189.2978515625\n",
      "65 5819.46875\n",
      "66 5474.27099609375\n",
      "67 5153.154296875\n",
      "68 4853.8662109375\n",
      "69 4573.87939453125\n",
      "70 4311.94091796875\n",
      "71 4066.6591796875\n",
      "72 3836.947021484375\n",
      "73 3621.600341796875\n",
      "74 3419.70654296875\n",
      "75 3230.320068359375\n",
      "76 3052.52294921875\n",
      "77 2885.52490234375\n",
      "78 2728.51708984375\n",
      "79 2580.9287109375\n",
      "80 2442.171630859375\n",
      "81 2311.59716796875\n",
      "82 2188.672607421875\n",
      "83 2072.916259765625\n",
      "84 1963.8359375\n",
      "85 1861.0848388671875\n",
      "86 1764.1343994140625\n",
      "87 1672.693115234375\n",
      "88 1586.426025390625\n",
      "89 1505.0030517578125\n",
      "90 1428.1263427734375\n",
      "91 1355.4959716796875\n",
      "92 1286.8704833984375\n",
      "93 1222.068115234375\n",
      "94 1160.78076171875\n",
      "95 1102.765625\n",
      "96 1047.886474609375\n",
      "97 995.9463500976562\n",
      "98 946.7880249023438\n",
      "99 900.2412719726562\n",
      "100 856.14501953125\n",
      "101 814.373046875\n",
      "102 774.7747192382812\n",
      "103 737.2532958984375\n",
      "104 701.6731567382812\n",
      "105 667.9257202148438\n",
      "106 635.9154052734375\n",
      "107 605.55078125\n",
      "108 576.7227783203125\n",
      "109 549.357421875\n",
      "110 523.3753662109375\n",
      "111 498.69964599609375\n",
      "112 475.2540283203125\n",
      "113 452.9850158691406\n",
      "114 431.8189697265625\n",
      "115 411.7001037597656\n",
      "116 392.57904052734375\n",
      "117 374.38836669921875\n",
      "118 357.0896911621094\n",
      "119 340.63360595703125\n",
      "120 324.9782409667969\n",
      "121 310.0980224609375\n",
      "122 295.92333984375\n",
      "123 282.4306335449219\n",
      "124 269.582275390625\n",
      "125 257.3511047363281\n",
      "126 245.70228576660156\n",
      "127 234.60545349121094\n",
      "128 224.03221130371094\n",
      "129 213.9578857421875\n",
      "130 204.35922241210938\n",
      "131 195.2102508544922\n",
      "132 186.4906005859375\n",
      "133 178.17608642578125\n",
      "134 170.25271606445312\n",
      "135 162.69720458984375\n",
      "136 155.4896697998047\n",
      "137 148.61221313476562\n",
      "138 142.0535430908203\n",
      "139 135.79705810546875\n",
      "140 129.8267822265625\n",
      "141 124.12980651855469\n",
      "142 118.69242858886719\n",
      "143 113.50205993652344\n",
      "144 108.54840087890625\n",
      "145 103.8194808959961\n",
      "146 99.30952453613281\n",
      "147 94.99662017822266\n",
      "148 90.87767791748047\n",
      "149 86.94279479980469\n",
      "150 83.18633270263672\n",
      "151 79.5966567993164\n",
      "152 76.1671142578125\n",
      "153 72.89053344726562\n",
      "154 69.75880432128906\n",
      "155 66.76749420166016\n",
      "156 63.90857696533203\n",
      "157 61.174644470214844\n",
      "158 58.56336975097656\n",
      "159 56.06591033935547\n",
      "160 53.677276611328125\n",
      "161 51.394775390625\n",
      "162 49.21134567260742\n",
      "163 47.12299728393555\n",
      "164 45.12712860107422\n",
      "165 43.21804428100586\n",
      "166 41.39107131958008\n",
      "167 39.644493103027344\n",
      "168 37.97233581542969\n",
      "169 36.375213623046875\n",
      "170 34.84648895263672\n",
      "171 33.383235931396484\n",
      "172 31.982177734375\n",
      "173 30.641164779663086\n",
      "174 29.35852813720703\n",
      "175 28.13058853149414\n",
      "176 26.956459045410156\n",
      "177 25.83295440673828\n",
      "178 24.758302688598633\n",
      "179 23.728702545166016\n",
      "180 22.743165969848633\n",
      "181 21.799667358398438\n",
      "182 20.896190643310547\n",
      "183 20.0308780670166\n",
      "184 19.20206642150879\n",
      "185 18.408586502075195\n",
      "186 17.64859390258789\n",
      "187 16.920814514160156\n",
      "188 16.223194122314453\n",
      "189 15.555269241333008\n",
      "190 14.915393829345703\n",
      "191 14.302596092224121\n",
      "192 13.715414047241211\n",
      "193 13.152823448181152\n",
      "194 12.613597869873047\n",
      "195 12.097203254699707\n",
      "196 11.601970672607422\n",
      "197 11.127365112304688\n",
      "198 10.6726713180542\n",
      "199 10.237316131591797\n",
      "200 9.81944465637207\n",
      "201 9.418939590454102\n",
      "202 9.035215377807617\n",
      "203 8.667389869689941\n",
      "204 8.314858436584473\n",
      "205 7.977268218994141\n",
      "206 7.653360843658447\n",
      "207 7.34268045425415\n",
      "208 7.044757843017578\n",
      "209 6.759425640106201\n",
      "210 6.48542594909668\n",
      "211 6.223141670227051\n",
      "212 5.9714884757995605\n",
      "213 5.7302165031433105\n",
      "214 5.498959064483643\n",
      "215 5.277109146118164\n",
      "216 5.064291000366211\n",
      "217 4.860201835632324\n",
      "218 4.664412498474121\n",
      "219 4.476502418518066\n",
      "220 4.29638147354126\n",
      "221 4.12351131439209\n",
      "222 3.957993984222412\n",
      "223 3.7990100383758545\n",
      "224 3.646592378616333\n",
      "225 3.5001542568206787\n",
      "226 3.36004900932312\n",
      "227 3.2253448963165283\n",
      "228 3.096252918243408\n",
      "229 2.9723129272460938\n",
      "230 2.85337495803833\n",
      "231 2.739318370819092\n",
      "232 2.630033254623413\n",
      "233 2.524862051010132\n",
      "234 2.4240622520446777\n",
      "235 2.3276052474975586\n",
      "236 2.2348358631134033\n",
      "237 2.145810127258301\n",
      "238 2.0604019165039062\n",
      "239 1.9784666299819946\n",
      "240 1.8998005390167236\n",
      "241 1.8241089582443237\n",
      "242 1.7516553401947021\n",
      "243 1.6820436716079712\n",
      "244 1.6152559518814087\n",
      "245 1.5511521100997925\n",
      "246 1.4896514415740967\n",
      "247 1.430523157119751\n",
      "248 1.3737775087356567\n",
      "249 1.319378137588501\n",
      "250 1.2671682834625244\n",
      "251 1.2170215845108032\n",
      "252 1.1688604354858398\n",
      "253 1.1227262020111084\n",
      "254 1.0783182382583618\n",
      "255 1.035767674446106\n",
      "256 0.9948843121528625\n",
      "257 0.9556346535682678\n",
      "258 0.91792231798172\n",
      "259 0.8816813230514526\n",
      "260 0.846973180770874\n",
      "261 0.8135503530502319\n",
      "262 0.7815559506416321\n",
      "263 0.75084388256073\n",
      "264 0.7213252782821655\n",
      "265 0.6929579377174377\n",
      "266 0.6656665205955505\n",
      "267 0.639559805393219\n",
      "268 0.6143907904624939\n",
      "269 0.5903188586235046\n",
      "270 0.5671483874320984\n",
      "271 0.5449157357215881\n",
      "272 0.5235095024108887\n",
      "273 0.5029903650283813\n",
      "274 0.48328179121017456\n",
      "275 0.46437498927116394\n",
      "276 0.4461865723133087\n",
      "277 0.42871835827827454\n",
      "278 0.41191837191581726\n",
      "279 0.39580464363098145\n",
      "280 0.38037362694740295\n",
      "281 0.3654544949531555\n",
      "282 0.351152241230011\n",
      "283 0.3375001549720764\n",
      "284 0.3243376612663269\n",
      "285 0.31165289878845215\n",
      "286 0.29951441287994385\n",
      "287 0.28779760003089905\n",
      "288 0.2765576243400574\n",
      "289 0.26581132411956787\n",
      "290 0.2554686665534973\n",
      "291 0.24550186097621918\n",
      "292 0.23598630726337433\n",
      "293 0.2267480194568634\n",
      "294 0.21793018281459808\n",
      "295 0.20947444438934326\n",
      "296 0.20133179426193237\n",
      "297 0.19350235164165497\n",
      "298 0.18598294258117676\n",
      "299 0.17879953980445862\n",
      "300 0.17187051475048065\n",
      "301 0.16519121825695038\n",
      "302 0.15878741443157196\n",
      "303 0.15263786911964417\n",
      "304 0.146720290184021\n",
      "305 0.14101943373680115\n",
      "306 0.13557909429073334\n",
      "307 0.13031069934368134\n",
      "308 0.12526655197143555\n",
      "309 0.12041234970092773\n",
      "310 0.11577080935239792\n",
      "311 0.11128769814968109\n",
      "312 0.10699911415576935\n",
      "313 0.10288065671920776\n",
      "314 0.0988907590508461\n",
      "315 0.09506750106811523\n",
      "316 0.09139177948236465\n",
      "317 0.08786045759916306\n",
      "318 0.08448150753974915\n",
      "319 0.08122096955776215\n",
      "320 0.07808570563793182\n",
      "321 0.07507353276014328\n",
      "322 0.07218538224697113\n",
      "323 0.06943099200725555\n",
      "324 0.06678102165460587\n",
      "325 0.06418174505233765\n",
      "326 0.061727024614810944\n",
      "327 0.05935472249984741\n",
      "328 0.057058800011873245\n",
      "329 0.054880622774362564\n",
      "330 0.05275510251522064\n",
      "331 0.05073384940624237\n",
      "332 0.04880383610725403\n",
      "333 0.04692675918340683\n",
      "334 0.045123711228370667\n",
      "335 0.043393295258283615\n",
      "336 0.0417277030646801\n",
      "337 0.040138378739356995\n",
      "338 0.03859812766313553\n",
      "339 0.03713270649313927\n",
      "340 0.03571578115224838\n",
      "341 0.03434634208679199\n",
      "342 0.033053215593099594\n",
      "343 0.03179135173559189\n",
      "344 0.030579302459955215\n",
      "345 0.02941129356622696\n",
      "346 0.028294675052165985\n",
      "347 0.027224794030189514\n",
      "348 0.026174673810601234\n",
      "349 0.025196624919772148\n",
      "350 0.024240829050540924\n",
      "351 0.02332492358982563\n",
      "352 0.022442733868956566\n",
      "353 0.02159499190747738\n",
      "354 0.02077660709619522\n",
      "355 0.019987866282463074\n",
      "356 0.01924194023013115\n",
      "357 0.018515687435865402\n",
      "358 0.01782488264143467\n",
      "359 0.01715306006371975\n",
      "360 0.016512325033545494\n",
      "361 0.01589464396238327\n",
      "362 0.015296089462935925\n",
      "363 0.014715977013111115\n",
      "364 0.014172167517244816\n",
      "365 0.0136414160951972\n",
      "366 0.013132796622812748\n",
      "367 0.012644987553358078\n",
      "368 0.012171880342066288\n",
      "369 0.011717738583683968\n",
      "370 0.011284895241260529\n",
      "371 0.01086580753326416\n",
      "372 0.010459515266120434\n",
      "373 0.010077586397528648\n",
      "374 0.009712251834571362\n",
      "375 0.009358481504023075\n",
      "376 0.009008223190903664\n",
      "377 0.008684639818966389\n",
      "378 0.008366063237190247\n",
      "379 0.008059750311076641\n",
      "380 0.007767942734062672\n",
      "381 0.007486106362193823\n",
      "382 0.007216602563858032\n",
      "383 0.006947552319616079\n",
      "384 0.006706681102514267\n",
      "385 0.006464706733822823\n",
      "386 0.00622725673019886\n",
      "387 0.006005025934427977\n",
      "388 0.005791414994746447\n",
      "389 0.005582955665886402\n",
      "390 0.005383046343922615\n",
      "391 0.005187425296753645\n",
      "392 0.005006624385714531\n",
      "393 0.004830698017030954\n",
      "394 0.004658902529627085\n",
      "395 0.004497162997722626\n",
      "396 0.0043393466621637344\n",
      "397 0.0041861701756715775\n",
      "398 0.004047679714858532\n",
      "399 0.003905466990545392\n",
      "400 0.0037718380335718393\n",
      "401 0.0036459474358707666\n",
      "402 0.0035205124877393246\n",
      "403 0.0033979052677750587\n",
      "404 0.0032835649326443672\n",
      "405 0.0031745275482535362\n",
      "406 0.0030660154297947884\n",
      "407 0.0029623566661030054\n",
      "408 0.0028655645437538624\n",
      "409 0.002771106082946062\n",
      "410 0.0026786900125443935\n",
      "411 0.0025919280014932156\n",
      "412 0.0025043075438588858\n",
      "413 0.0024256438482552767\n",
      "414 0.002346136374399066\n",
      "415 0.0022685339208692312\n",
      "416 0.002196434885263443\n",
      "417 0.0021255137398838997\n",
      "418 0.002059517428278923\n",
      "419 0.0019923653453588486\n",
      "420 0.0019316681427881122\n",
      "421 0.0018696710467338562\n",
      "422 0.0018129756208509207\n",
      "423 0.001754236058332026\n",
      "424 0.0017018549842759967\n",
      "425 0.0016496321186423302\n",
      "426 0.0016004376811906695\n",
      "427 0.0015528136864304543\n",
      "428 0.001506053376942873\n",
      "429 0.0014621159061789513\n",
      "430 0.0014180204598233104\n",
      "431 0.001375424675643444\n",
      "432 0.0013350732624530792\n",
      "433 0.001295066555030644\n",
      "434 0.0012577804736793041\n",
      "435 0.0012212323490530252\n",
      "436 0.0011848369613289833\n",
      "437 0.0011501595145091414\n",
      "438 0.0011170500656589866\n",
      "439 0.0010853399289771914\n",
      "440 0.0010541558731347322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 0.001026207348331809\n",
      "442 0.0009982767514884472\n",
      "443 0.0009712513419799507\n",
      "444 0.0009454565588384867\n",
      "445 0.0009193280711770058\n",
      "446 0.0008941139094531536\n",
      "447 0.0008690176764503121\n",
      "448 0.0008447642321698368\n",
      "449 0.0008226071367971599\n",
      "450 0.0008006204734556377\n",
      "451 0.0007803522166796029\n",
      "452 0.0007596786599606276\n",
      "453 0.0007403125055134296\n",
      "454 0.000720523705240339\n",
      "455 0.0007014659349806607\n",
      "456 0.0006846809410490096\n",
      "457 0.000666445994284004\n",
      "458 0.0006497877766378224\n",
      "459 0.0006331478944048285\n",
      "460 0.000618316582404077\n",
      "461 0.000602094572968781\n",
      "462 0.00058671401347965\n",
      "463 0.0005720214685425162\n",
      "464 0.00055790098849684\n",
      "465 0.0005452103214338422\n",
      "466 0.0005318432813510299\n",
      "467 0.0005183257162570953\n",
      "468 0.0005062907002866268\n",
      "469 0.0004940853104926646\n",
      "470 0.00048301785136573017\n",
      "471 0.00047187056043185294\n",
      "472 0.0004607629089150578\n",
      "473 0.0004493992018979043\n",
      "474 0.0004401388578116894\n",
      "475 0.00042996712727472186\n",
      "476 0.0004201772389933467\n",
      "477 0.0004104323161300272\n",
      "478 0.0004019253246951848\n",
      "479 0.00039248907705768943\n",
      "480 0.0003829729394055903\n",
      "481 0.00037452843389473855\n",
      "482 0.0003668524732347578\n",
      "483 0.0003588016261346638\n",
      "484 0.00035038674832321703\n",
      "485 0.00034250522730872035\n",
      "486 0.00033586728386580944\n",
      "487 0.00032888533314689994\n",
      "488 0.00032183091389015317\n",
      "489 0.00031505885999649763\n",
      "490 0.0003081467002630234\n",
      "491 0.0003023558820132166\n",
      "492 0.00029684294713661075\n",
      "493 0.00029031556914560497\n",
      "494 0.0002849168668035418\n",
      "495 0.0002784930693451315\n",
      "496 0.00027272276929579675\n",
      "497 0.00026770838303491473\n",
      "498 0.000261798151768744\n",
      "499 0.00025702177663333714\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 647.4382934570312\n",
      "1 600.087158203125\n",
      "2 559.1369018554688\n",
      "3 523.3048706054688\n",
      "4 491.38543701171875\n",
      "5 462.6326904296875\n",
      "6 436.7915954589844\n",
      "7 412.8688049316406\n",
      "8 390.6138000488281\n",
      "9 369.7791748046875\n",
      "10 350.4232177734375\n",
      "11 332.2376403808594\n",
      "12 315.030029296875\n",
      "13 298.8269958496094\n",
      "14 283.39678955078125\n",
      "15 268.62750244140625\n",
      "16 254.55262756347656\n",
      "17 241.0894775390625\n",
      "18 228.19163513183594\n",
      "19 215.86724853515625\n",
      "20 204.15647888183594\n",
      "21 192.9982452392578\n",
      "22 182.3338623046875\n",
      "23 172.0997314453125\n",
      "24 162.33778381347656\n",
      "25 153.04444885253906\n",
      "26 144.18792724609375\n",
      "27 135.77603149414062\n",
      "28 127.79715728759766\n",
      "29 120.20848846435547\n",
      "30 113.01699829101562\n",
      "31 106.19905090332031\n",
      "32 99.7492904663086\n",
      "33 93.66423034667969\n",
      "34 87.92057037353516\n",
      "35 82.48326110839844\n",
      "36 77.35348510742188\n",
      "37 72.52841186523438\n",
      "38 67.99259185791016\n",
      "39 63.725608825683594\n",
      "40 59.71083068847656\n",
      "41 55.94647216796875\n",
      "42 52.4238395690918\n",
      "43 49.12247085571289\n",
      "44 46.03453063964844\n",
      "45 43.14530563354492\n",
      "46 40.44254684448242\n",
      "47 37.91310501098633\n",
      "48 35.550743103027344\n",
      "49 33.3394775390625\n",
      "50 31.27054786682129\n",
      "51 29.33942985534668\n",
      "52 27.53679084777832\n",
      "53 25.852153778076172\n",
      "54 24.279155731201172\n",
      "55 22.810955047607422\n",
      "56 21.441776275634766\n",
      "57 20.162433624267578\n",
      "58 18.96467399597168\n",
      "59 17.84571647644043\n",
      "60 16.798139572143555\n",
      "61 15.819026947021484\n",
      "62 14.903908729553223\n",
      "63 14.04746150970459\n",
      "64 13.246447563171387\n",
      "65 12.49626636505127\n",
      "66 11.795320510864258\n",
      "67 11.137409210205078\n",
      "68 10.52103042602539\n",
      "69 9.943305969238281\n",
      "70 9.400752067565918\n",
      "71 8.891609191894531\n",
      "72 8.412918090820312\n",
      "73 7.961352348327637\n",
      "74 7.537214756011963\n",
      "75 7.13818359375\n",
      "76 6.763121604919434\n",
      "77 6.410325527191162\n",
      "78 6.077510833740234\n",
      "79 5.764362812042236\n",
      "80 5.469998359680176\n",
      "81 5.192549228668213\n",
      "82 4.930924415588379\n",
      "83 4.684165000915527\n",
      "84 4.451141357421875\n",
      "85 4.231402397155762\n",
      "86 4.023688316345215\n",
      "87 3.827932596206665\n",
      "88 3.642942428588867\n",
      "89 3.468083381652832\n",
      "90 3.3024582862854004\n",
      "91 3.145742177963257\n",
      "92 2.996516227722168\n",
      "93 2.85524320602417\n",
      "94 2.7214515209198\n",
      "95 2.594539165496826\n",
      "96 2.4743173122406006\n",
      "97 2.36018705368042\n",
      "98 2.2519805431365967\n",
      "99 2.14924693107605\n",
      "100 2.0516748428344727\n",
      "101 1.9590153694152832\n",
      "102 1.871117353439331\n",
      "103 1.7876214981079102\n",
      "104 1.7082154750823975\n",
      "105 1.6327316761016846\n",
      "106 1.5610419511795044\n",
      "107 1.4929213523864746\n",
      "108 1.4280592203140259\n",
      "109 1.366273283958435\n",
      "110 1.3074326515197754\n",
      "111 1.2513377666473389\n",
      "112 1.19795560836792\n",
      "113 1.1471277475357056\n",
      "114 1.0986217260360718\n",
      "115 1.0524014234542847\n",
      "116 1.0085127353668213\n",
      "117 0.9666956663131714\n",
      "118 0.9267896413803101\n",
      "119 0.8887162208557129\n",
      "120 0.8523494005203247\n",
      "121 0.8176078796386719\n",
      "122 0.784426212310791\n",
      "123 0.7527914047241211\n",
      "124 0.722602128982544\n",
      "125 0.6937674880027771\n",
      "126 0.6662439703941345\n",
      "127 0.639921247959137\n",
      "128 0.6147875785827637\n",
      "129 0.590749204158783\n",
      "130 0.567730188369751\n",
      "131 0.545720100402832\n",
      "132 0.5246472954750061\n",
      "133 0.5045124888420105\n",
      "134 0.48526403307914734\n",
      "135 0.4668435752391815\n",
      "136 0.44920995831489563\n",
      "137 0.43231287598609924\n",
      "138 0.4161142110824585\n",
      "139 0.40061575174331665\n",
      "140 0.3857313394546509\n",
      "141 0.3714464604854584\n",
      "142 0.3577578365802765\n",
      "143 0.34462404251098633\n",
      "144 0.33202800154685974\n",
      "145 0.31993770599365234\n",
      "146 0.3083477020263672\n",
      "147 0.2972317337989807\n",
      "148 0.28656473755836487\n",
      "149 0.2763234078884125\n",
      "150 0.26648667454719543\n",
      "151 0.257049024105072\n",
      "152 0.2479785978794098\n",
      "153 0.23926231265068054\n",
      "154 0.23088309168815613\n",
      "155 0.22282692790031433\n",
      "156 0.2150895893573761\n",
      "157 0.20764891803264618\n",
      "158 0.20048899948596954\n",
      "159 0.19360263645648956\n",
      "160 0.18698588013648987\n",
      "161 0.1806221455335617\n",
      "162 0.1744920313358307\n",
      "163 0.16859011352062225\n",
      "164 0.162916362285614\n",
      "165 0.15744836628437042\n",
      "166 0.15218408405780792\n",
      "167 0.14711476862430573\n",
      "168 0.1422458440065384\n",
      "169 0.1375623494386673\n",
      "170 0.13305121660232544\n",
      "171 0.1287047564983368\n",
      "172 0.12451574951410294\n",
      "173 0.12047827243804932\n",
      "174 0.11658445000648499\n",
      "175 0.11283189058303833\n",
      "176 0.10921338945627213\n",
      "177 0.10572279989719391\n",
      "178 0.10236065089702606\n",
      "179 0.09911572188138962\n",
      "180 0.09598525613546371\n",
      "181 0.09296397864818573\n",
      "182 0.09004876017570496\n",
      "183 0.08723504841327667\n",
      "184 0.08452310413122177\n",
      "185 0.08188609778881073\n",
      "186 0.07934065908193588\n",
      "187 0.0768829882144928\n",
      "188 0.0745123103260994\n",
      "189 0.07222286611795425\n",
      "190 0.0700126439332962\n",
      "191 0.06787769496440887\n",
      "192 0.06581703573465347\n",
      "193 0.06382746249437332\n",
      "194 0.06190464273095131\n",
      "195 0.060046181082725525\n",
      "196 0.05825049430131912\n",
      "197 0.05651463568210602\n",
      "198 0.054838214069604874\n",
      "199 0.053217463195323944\n",
      "200 0.05165024474263191\n",
      "201 0.050135254859924316\n",
      "202 0.04866894707083702\n",
      "203 0.04725265130400658\n",
      "204 0.04588392376899719\n",
      "205 0.0445593036711216\n",
      "206 0.04327758029103279\n",
      "207 0.04203891009092331\n",
      "208 0.04083951935172081\n",
      "209 0.039682790637016296\n",
      "210 0.03856242448091507\n",
      "211 0.03747735545039177\n",
      "212 0.036426808685064316\n",
      "213 0.03540954366326332\n",
      "214 0.03442477807402611\n",
      "215 0.033470191061496735\n",
      "216 0.03254600986838341\n",
      "217 0.03165121003985405\n",
      "218 0.030783813446760178\n",
      "219 0.029942872002720833\n",
      "220 0.029128191992640495\n",
      "221 0.028338270261883736\n",
      "222 0.027573132887482643\n",
      "223 0.026830818504095078\n",
      "224 0.02611153945326805\n",
      "225 0.025414763018488884\n",
      "226 0.024739785119891167\n",
      "227 0.02408451773226261\n",
      "228 0.023448267951607704\n",
      "229 0.022831175476312637\n",
      "230 0.02223246730864048\n",
      "231 0.021651610732078552\n",
      "232 0.021087875589728355\n",
      "233 0.02054089866578579\n",
      "234 0.020009782165288925\n",
      "235 0.019493967294692993\n",
      "236 0.01899331621825695\n",
      "237 0.018507219851017\n",
      "238 0.01803542487323284\n",
      "239 0.017576906830072403\n",
      "240 0.017131829634308815\n",
      "241 0.01669955998659134\n",
      "242 0.01627940870821476\n",
      "243 0.015871195122599602\n",
      "244 0.015474549494683743\n",
      "245 0.015089008025825024\n",
      "246 0.014714363031089306\n",
      "247 0.014350098557770252\n",
      "248 0.01399619784206152\n",
      "249 0.013652282766997814\n",
      "250 0.013317684642970562\n",
      "251 0.012992385774850845\n",
      "252 0.012675886042416096\n",
      "253 0.012368284165859222\n",
      "254 0.012069051153957844\n",
      "255 0.011777881532907486\n",
      "256 0.011494772508740425\n",
      "257 0.011219318024814129\n",
      "258 0.010951370000839233\n",
      "259 0.010690547525882721\n",
      "260 0.010436753742396832\n",
      "261 0.010189700871706009\n",
      "262 0.009949350729584694\n",
      "263 0.009715289808809757\n",
      "264 0.00948738306760788\n",
      "265 0.009265593253076077\n",
      "266 0.009049667976796627\n",
      "267 0.008839335292577744\n",
      "268 0.008634443394839764\n",
      "269 0.00843488797545433\n",
      "270 0.00824047438800335\n",
      "271 0.008051189593970776\n",
      "272 0.007866784930229187\n",
      "273 0.007687048055231571\n",
      "274 0.007512033451348543\n",
      "275 0.0073413485661149025\n",
      "276 0.0071750725619494915\n",
      "277 0.00701302383095026\n",
      "278 0.006855071987956762\n",
      "279 0.006701045669615269\n",
      "280 0.006550964899361134\n",
      "281 0.006404636427760124\n",
      "282 0.006261912640184164\n",
      "283 0.0061227986589074135\n",
      "284 0.005987093318253756\n",
      "285 0.005854753777384758\n",
      "286 0.005725707393139601\n",
      "287 0.005599787458777428\n",
      "288 0.005477039609104395\n",
      "289 0.0053572445176541805\n",
      "290 0.005240363534539938\n",
      "291 0.005126357544213533\n",
      "292 0.005015084054321051\n",
      "293 0.004906530026346445\n",
      "294 0.004800593946129084\n",
      "295 0.004697201773524284\n",
      "296 0.0045962855219841\n",
      "297 0.004497794434428215\n",
      "298 0.004401675891131163\n",
      "299 0.004307817667722702\n",
      "300 0.004216211382299662\n",
      "301 0.004126745741814375\n",
      "302 0.004039325751364231\n",
      "303 0.003953986801207066\n",
      "304 0.003870666027069092\n",
      "305 0.003789253532886505\n",
      "306 0.0037097809836268425\n",
      "307 0.0036321307998150587\n",
      "308 0.0035562575794756413\n",
      "309 0.0034821261651813984\n",
      "310 0.0034097174648195505\n",
      "311 0.0033389702439308167\n",
      "312 0.0032698693685233593\n",
      "313 0.0032023009844124317\n",
      "314 0.003136287909001112\n",
      "315 0.00307177915237844\n",
      "316 0.003008742118254304\n",
      "317 0.0029471099842339754\n",
      "318 0.002886873437091708\n",
      "319 0.002827990334481001\n",
      "320 0.0027704453095793724\n",
      "321 0.0027141610626131296\n",
      "322 0.0026591282803565264\n",
      "323 0.0026053295005112886\n",
      "324 0.0025527221150696278\n",
      "325 0.002501268871128559\n",
      "326 0.0024509525392204523\n",
      "327 0.0024017503019422293\n",
      "328 0.002353630494326353\n",
      "329 0.002306553302332759\n",
      "330 0.0022605103440582752\n",
      "331 0.0022154548205435276\n",
      "332 0.002171402331441641\n",
      "333 0.0021282907109707594\n",
      "334 0.002086103428155184\n",
      "335 0.002044856082648039\n",
      "336 0.0020044639240950346\n",
      "337 0.0019649709574878216\n",
      "338 0.0019262993009760976\n",
      "339 0.0018884504679590464\n",
      "340 0.0018514300463721156\n",
      "341 0.0018151768017560244\n",
      "342 0.001779714715667069\n",
      "343 0.001745001063682139\n",
      "344 0.0017110274638980627\n",
      "345 0.0016777594573795795\n",
      "346 0.001645185286179185\n",
      "347 0.0016132990131154656\n",
      "348 0.0015820786356925964\n",
      "349 0.0015515200793743134\n",
      "350 0.0015215950552374125\n",
      "351 0.0014922915725037456\n",
      "352 0.0014636121923103929\n",
      "353 0.0014354977756738663\n",
      "354 0.0014079863904044032\n",
      "355 0.0013810514938086271\n",
      "356 0.001354648731648922\n",
      "357 0.0013288065092638135\n",
      "358 0.001303474185988307\n",
      "359 0.0012786724837496877\n",
      "360 0.0012543747434392571\n",
      "361 0.0012305784039199352\n",
      "362 0.0012072615791112185\n",
      "363 0.0011844184482470155\n",
      "364 0.0011620423756539822\n",
      "365 0.001140108797699213\n",
      "366 0.0011186393676325679\n",
      "367 0.0010975829791277647\n",
      "368 0.0010769573273137212\n",
      "369 0.0010567507706582546\n",
      "370 0.0010369422379881144\n",
      "371 0.0010175302159041166\n",
      "372 0.0009985185461118817\n",
      "373 0.000979878823272884\n",
      "374 0.0009615968447178602\n",
      "375 0.0009436890832148492\n",
      "376 0.0009261452360078692\n",
      "377 0.0009089296800084412\n",
      "378 0.0008920683176256716\n",
      "379 0.0008755443850532174\n",
      "380 0.0008593285456299782\n",
      "381 0.0008434479823336005\n",
      "382 0.0008278776076622307\n",
      "383 0.0008126093889586627\n",
      "384 0.0007976319757290184\n",
      "385 0.0007829621317796409\n",
      "386 0.0007685691234655678\n",
      "387 0.0007544580148532987\n",
      "388 0.0007406207150779665\n",
      "389 0.0007270535570569336\n",
      "390 0.0007137514185160398\n",
      "391 0.0007007056847214699\n",
      "392 0.0006879120483063161\n",
      "393 0.0006753614288754761\n",
      "394 0.0006630491116084158\n",
      "395 0.0006509893573820591\n",
      "396 0.0006391489878296852\n",
      "397 0.0006275398191064596\n",
      "398 0.0006161460769362748\n",
      "399 0.0006049762014299631\n",
      "400 0.0005940177943557501\n",
      "401 0.0005832755123265088\n",
      "402 0.0005727371899411082\n",
      "403 0.0005623963079415262\n",
      "404 0.0005522464052774012\n",
      "405 0.0005422921967692673\n",
      "406 0.0005325282108969986\n",
      "407 0.0005229449598118663\n",
      "408 0.0005135525134392083\n",
      "409 0.0005043282289989293\n",
      "410 0.0004952828749082983\n",
      "411 0.00048641080502420664\n",
      "412 0.00047771132085472345\n",
      "413 0.00046916332212276757\n",
      "414 0.0004607731243595481\n",
      "415 0.00045254474389366806\n",
      "416 0.0004444703517947346\n",
      "417 0.00043654616456478834\n",
      "418 0.0004287648480385542\n",
      "419 0.00042113554081879556\n",
      "420 0.00041364741628058255\n",
      "421 0.00040629610884934664\n",
      "422 0.0003990844124928117\n",
      "423 0.00039199739694595337\n",
      "424 0.00038505037082359195\n",
      "425 0.0003782288113143295\n",
      "426 0.00037153559969738126\n",
      "427 0.0003649630816653371\n",
      "428 0.00035851445863954723\n",
      "429 0.00035218021366745234\n",
      "430 0.0003459676227066666\n",
      "431 0.0003398621629457921\n",
      "432 0.0003338755923323333\n",
      "433 0.0003279977245256305\n",
      "434 0.0003222287632524967\n",
      "435 0.0003165616362821311\n",
      "436 0.00031099983607418835\n",
      "437 0.0003055421111639589\n",
      "438 0.00030018005054444075\n",
      "439 0.0002949161862488836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440 0.0002897505182772875\n",
      "441 0.000284675566945225\n",
      "442 0.00027969543589279056\n",
      "443 0.0002748037804849446\n",
      "444 0.0002700034820009023\n",
      "445 0.00026528595481067896\n",
      "446 0.0002606570196803659\n",
      "447 0.0002561100700404495\n",
      "448 0.0002516453678254038\n",
      "449 0.0002472636988386512\n",
      "450 0.0002429581363685429\n",
      "451 0.00023873501049820334\n",
      "452 0.00023458605573978275\n",
      "453 0.0002305114467162639\n",
      "454 0.0002265045914100483\n",
      "455 0.000222574730287306\n",
      "456 0.00021871317585464567\n",
      "457 0.00021492387168109417\n",
      "458 0.0002112004440277815\n",
      "459 0.0002075426746159792\n",
      "460 0.00020395124738570303\n",
      "461 0.00020042344112880528\n",
      "462 0.00019695931405294687\n",
      "463 0.0001935586624313146\n",
      "464 0.00019021652406081557\n",
      "465 0.0001869318075478077\n",
      "466 0.00018370836914982647\n",
      "467 0.0001805452338885516\n",
      "468 0.0001774332922650501\n",
      "469 0.00017438600480090827\n",
      "470 0.00017138404655270278\n",
      "471 0.00016843447519931942\n",
      "472 0.00016553675231989473\n",
      "473 0.00016269527259282768\n",
      "474 0.00015989858366083354\n",
      "475 0.00015715276822447777\n",
      "476 0.0001544562546769157\n",
      "477 0.0001518100471002981\n",
      "478 0.0001492063165642321\n",
      "479 0.00014665015623904765\n",
      "480 0.0001441385393263772\n",
      "481 0.0001416712620994076\n",
      "482 0.00013924662198405713\n",
      "483 0.0001368637167615816\n",
      "484 0.0001345256168860942\n",
      "485 0.00013222948473412544\n",
      "486 0.0001299677387578413\n",
      "487 0.0001277511182706803\n",
      "488 0.0001255726965609938\n",
      "489 0.0001234289666172117\n",
      "490 0.00012132353003835306\n",
      "491 0.00011925887520192191\n",
      "492 0.00011722678027581424\n",
      "493 0.00011522755085024983\n",
      "494 0.00011326966341584921\n",
      "495 0.00011134002124890685\n",
      "496 0.0001094484978239052\n",
      "497 0.00010758780263131484\n",
      "498 0.00010575843043625355\n",
      "499 0.00010396007564850152\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    #print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015719758150219356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f177a1ccf60>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.loadtxt('fitting.txt')\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "x1 , y1 = data[:,0],data[:,1]\n",
    "x,y = torch.from_numpy(np.asmatrix(x1).T),torch.from_numpy(np.asmatrix(y1).T)\n",
    "plt.scatter(x1,y1, label = 'data')\n",
    "#x, y = x1.double(), y1.double()\n",
    "\n",
    "\n",
    "N, D_in, H, D_out = len(x), 1, 350, 1\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(5000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    #print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "plt.scatter(x,y_pred.data, label = 'pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
